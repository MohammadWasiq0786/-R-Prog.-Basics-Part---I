\documentclass[a4paper,12pt]{report}
\title{\textbf{Unit III(STB555)---Simulation and Fitting of Distributions}}
\author{Athar Ali Khan (Email: atharkhan1962@gmail.com)\\Department of Statistics and O R\\Aligarh Muslim University, Aligarh.}
\begin{document}
\maketitle
\tableofcontents
\listoffigures
\chapter{Random number generation}
In this chapter we shall study about the \textbf{R} implementations for random number generation and sampling procedures. Fitting of polynomials and exponential curves. Application problems based on fitting of suitable distributions. Normal probability plot.
\section{Rndom number generation from uniform $U(a,b)$}
Random numbers can be generated by the function \texttt{runif()}. A general syntax is:
\begin{verbatim}
runif(n,min=a,max=b) 
\end{verbatim}
Execution of this command produces $n$ pseudo random uniform numbers on the interval $[a, b]$. The default values are $a=0$, and $b=1$. The seed is selected internally. However, one can set the seed by using \texttt{set.seed()} function.

\subsection{Example unifrom deviates}
Generate $5$ uniform pseudo random numbers on the interval $[0, 1]$, and $10$ uniform such numbers on the interval $[-3,-1]$.
<< unif1, message=FALSE,warning=FALSE>>=
runif(5) # same as runif(5,0,1)
runif(10,min=-3,max=-1)
@
One can use \texttt{set.seed()} to generate the same number every time.
<<runifsetseed1>>=
set.seed(1)
runif(5)
set.seed(1)
runif(10,-3,-1)
@
\subsection{Exercise: random deviates from $U(a,b)$}
Use \texttt{runif()} with \texttt{set.seed(32078)} to generate 10 pseudo random numbers from:
\begin{itemize}
\item[(a)] the uniform $U(0,1)$ distribution
\item[(b)] the uniform $U(3,7)$ distribution
\item[(c)] the uniform $U(-2,2)$ distribution.
\end{itemize}
\subsection{Example on application of simulation}
Generate 1000 uniform random variate using \texttt{runif()} function, assigning them to a vector called \texttt{U}. Do the following:
\begin{itemize}
\item[(a)] Compute the average, variance,and standard deviation of the numbers in \texttt{U}.
\item[(b)] Compare your results with true mean, variance, and standard deviation.
\item[(c)] Compute the proportion of values of \texttt{U} that are less than 0.6,and compare with the probability that uniform random variable \texttt{U} is less than 0.6.
\item[(d)] Estimate the expected value of $\frac{1}{U+1}$.
\end{itemize}
\textbf{Solution using R commands}
<<solution1>>=
set.seed(19908)
U<-runif(1000)
# Part (a)
mean(U) # it should be 0.5
var(U) # it should be 1/12.
sd(U)  # it should be sqrt(1/12)
# Part (c)
# Note that U<0.6 returns a logical vector, Thus
mean(U<0.6) # same as Prob(U<0.6)=punif(0.6), F(x)=x for uniform.
# Part (d) E(1/(U+1)) is
mean(1/(U+1))
# histogram of U and x=1/(U+1)
opar<-par(mfrow=c(1,2))
hist(U,prob=TRUE)
hist(1/(U+1),prob=TRUE)
par(opar)
@

Note that the distribution of $U$ is uniform whereas distribution of $\frac{1}{U+1}$ is more close to exponential distribution. These facts will be discussed in more detail in the later part of the course.

\subsection{Exercise on uniform $U(3.7,5.8)$}
Simulate 10000 independent observations on a uniformly distributed random variable on the interval $[3.7,5.8]$.
\begin{itemize}
\item[(a)] Estimate the mean, variance, and standard deviation of such a uniform random variable and compare your estimates with the true values.
\item[(b)] Estimate the probability that such a random variable is greater than 4. Compare it with true value.
\end{itemize}

\section{The \texttt{sample()} function}
The \texttt{sample()} function allows you to take a simple random sample from a vector of values. For example, 
\begin{verbatim}
sample(c(3,5,7),size=2,replace=FALSE)
\end{verbatim}
will yield a vector of two values taken (without replacement) from the set \texttt{\{3,5,3\}}. Use the function \texttt{sample()} to generate 50 pseudo random integers from 1 through 100:
\begin{itemize}
\item[(a)] sample without replacement
\item[(b)] sample with replacement.
\end{itemize}
\textbf{Solution}
<<sample1>>=
# Part (a)
x<-sample(1:100,size=50) #without replacement
x
# part (b)
y<-sample(1:100,size = 50,replace = TRUE) #with replacement
y
# Note that in y repetitions are allowed, whereas it is not in x.

## The function sample is also used for randomization
sample(c("A","B","C")) #returns randomization.
@
\section{Distributions in \textbf{R}}
Most of the distributions are implemented in \textbf{base R}. Remaining are implemented in other packages.

\subsection{Normal distribution $N(\mu,\sigma)$}
There four main aspects of a distribution which are implemented in R. They are:
\begin{itemize}
\item \texttt{dnorm(x,mean,sd)}, this is the value of the distribution with \texttt{mean} and \texttt{sd}. 
\item \texttt{pnorm(q,mean,sd)}, the value of the CDF at $q$. You know that CDF is defined as $P(X<q)=F(q)$.
\item \texttt{qnorm(p,mean,sd)}, this is the value of quantile at probability $p$. Note that quantile is the inverse image of CDF.
\item \texttt{rnorm(n,mean,sd)}, stands for random simulations from $N(\mu,\sigma)$.
\end{itemize}
We are illustrating them using \textbf{R} commandss:
<<dens1,message=FALSE,warning=FALSE,fig.cap="Standard Normal Distribution">>=
# Suppose we want to plot N(0,1).
curve(dnorm(x,mean=0,sd=1),from=-3,to=3)
# Value of density at 0
dnorm(0) # same as sqrt(2*pi)
1/sqrt(2*pi)
abline(v=0,lwd=2)
#value of cdf at 1.96
pnorm(1.96)
abline(v=1.96)
pnorm(-1.96)
abline(v=-1.96)
# Note that area between -1.96 and 1.96 is 0.95
pnorm(1.96)-pnorm(-1.96)
# What should be pnorm(0), it should be 0.5
pnorm(0)
## An example of qnorm(.5)
qnorm(0.5)
qnorm(0.975)
qnorm(0.025)
## An example of 10 random simulation from N(0,1)
rnorm(n=10,mean=0,sd=1)
@

\subsection{Exponential distribution}
Like normal we can also define exponential distribution:
\begin{verbatim}
dexp(x,rate) #value of density
pexp(q,rate) #value of cdf
qexp(p,rate) #value of quantile
rexp(n,rate) # random simulation from exponential
\end{verbatim}
<<expr1,message=FALSE,warning=FALSE,fig.cap="Standard Exponential distribution">>=
re<-rexp(n=10000,rate=1) #standard exponential
mean(re)
sd(re)
#compute Pr(re>3)
mean(re>3) # probabilty re>3, through si,ulation
# Check it using pexp
pexp(3,lower.tail = FALSE) #exact
hist(re,prob=TRUE)
curve(dexp(x,rate=1),add=TRUE,lwd=2)
@
Since histogram is made from simulated data and curve is exact, but both are matching. This implies that our simulation is correct.
\section{Inverse CDF method of simulation}
It is a well known result of probability that
$$If\quad X\sim F,cdf\quad of \quad X$$
then
$$F(X)\sim U(0,1)$$
This implies that one can simulate quantiles from $F$ by using the following algorithm:
\begin{itemize}
\item[(i)] Equal the cdf $F(x)=u$ and solve it for $x$, where $u$ is $U(0,1)$ deviate.
\item[(ii)] The equation $x=F^{-1}(u)$ will be a quantile from $F$.
\end{itemize}
\subsection{Simulation from exponential distribution}
We can make use of \emph{Inverse CDF} method for simulation from exponential distribution with $rate=\lambda$. Note that
\begin{equation}
f(x)=\lambda e^{-\lambda x}\quad x>0
\Rightarrow F(x)=1- e^{-\lambda x}
\end{equation}
Thus,
\begin{equation}
 F(x)=u
\Rightarrow 1-e^{-\lambda x}=u
\Rightarrow 1-u=e^{-\lambda x}
\Rightarrow log(1-u)=-\lambda x
\Rightarrow x=-\frac{1}{\lambda}log(1-u)
\end{equation}
Thus \textbf{R} commands are:
<<rexp1,message=FALSE,warning=FALSE,fig.cap="Exponential density with mean 50 or rate=0.02">>=
u<-runif(1000)
lambda<-.02 # value of rate
x<--(1/lambda)*log(1-u) # simulated quantiles
#Check for mean, 1/0.02=50
mean(x) #1/lambda
var(x) #1/lambda^2
sd(x) #1/lambda
mean(x>50) #Prob(X>50)
hist(x,prob=TRUE)
curve(dexp(x,rate=.02),add=TRUE,lwd=2)
abline(v=50,col="red",lwd=2) # vertical line at mean
@
Note that simulated results obtained by using \emph{Inverse CDF} method are matching with the exact results, both in terms of numeric as well as graphics. 

\subsection{Exercise on Simulation from Possion}\label{pois1}
Estimate the mean and variance of a Poisson random variable whose mean is $7.2$ by simulating $10000$ Poisson pseudo random numbers. Compare your results with theoretical results. (Hint \texttt{rpois(n,lambda)}).

\subsection{Exercise on simulation from $N(0,1)$}\label{norm01}
Simulate $10000$ realizations of standard normal random variable $Z$, and use your simulated sample to estimate:
\begin{itemize}
\item[(a)] $P(Z>2.5)$
\item[(b)] $P(0<Z<1.645)$
\item[(c)] $P(1.2<Z<1.45)$
\item[(d)] $P(-1.2<Z<1.3)$
\end{itemize}
Compare your results with exact ones using \texttt{pnorm()}.\\
Hint: 
\begin{verbatim}
Z<-rnorm(10000)
(a) mean(Z>2.5)
(b) mean(Z>0 & Z<1.645)
\end{verbatim}

\subsection{Exercise on Simulation from $\chi^2$ distribution}
Simulate $1000$ realizations of a $\chi_{(8)}^2$, and estimate its mean and variance. Compare it with theoretical mean (8) and variance (16). Note that for $\chi_{(k)}^2$ the mean is $k$ and
variance is $2k$. Make a histogram of simulated data and overlay exact density curve on it.

Hint: 
\begin{verbatim}
x<-rchisq(n=1000,df=8)
mean(x)
var(x)
\end{verbatim}

\subsection{Exercise on simulation from binomial}\label{binom_norm}
Simulate 1000 observations from $binomial(10,0.5)$. Compute its mean and variance,and compare it with true mean($10\times 0.5$) and variance $(10\times 0.5\times 0.5)$.
\begin{itemize}
\item[(a)] Make a histogram of simulated observations, and add a Normal curve $N(5,2.5)$ on it.
\item[(b)] Comment on the closeness of histogram and Normal curve. Do you feel that binomial probability can be approximated by Normal probability? 
\end{itemize}

Hint:
\begin{verbatim}
x<-rbinom(n=1000,size=10,prob=0.5)
mean(x)
var(x)
\end{verbatim}
\section{Law of large numbers}
It states that as 
\begin{equation}
 n\rightarrow \infty \Rightarrow \bar{x}\stackrel{p}\rightarrow \mu
\end{equation}
Thus for large sample size, sample mean ($\bar{x}$) converges to population mean ($\mu$).
\subsection{Height of American women}
In this example we simulate heights of American women from $N(64.5, 2.5)$ try to see the behavior of sample mean which converges to population mean as sample size becomes large. This phenomenon has been illustrated in the following \textbf{R} commands.
\begin{figure}
\begin{center}
<<lawlarge1,messgae=FALSE,warning=FALSE>>=
#generate sample means for different sample sizes.
xbar<-sapply(1:10000, function(n) mean(rnorm(n,64.5,2.5)))
plot(xbar,xlim=c(0,10000))
abline(h=64.5,lwd=2,col="blue")
@
\end{center}
\caption[Law of large numbers]{Note that as sample size increases,that is $n \rightarrow \infty$, sample mean converges to population mean (64.5),that is, $\bar{x}\stackrel{p}\rightarrow \mu$, which is shown by the blue horizontal line. Note that after $n=4000$ sample mean is converging around the population mean $64.5$.}
\label{lln1}
\end{figure}

Note that we have used a new function \texttt{sapply()} whose first argument is a vector or listed data and second argument is the expression which calls the first argument repeatedly. For example, in the above situation \texttt{sapply()} repeatedly calculates mean of random sample of sizes $1:10000$. The graphic output is reported in Figure~\ref{lln1}.

\section{Sampling distribution---\texttt{replicate()}}
Note that \emph{statistic} is a function of sampled values, and distribution of a statistic is called \emph{sampling distribution}. In \textbf{R}, the function \texttt{replicate()} is an implementation of sampling distribution. Its syntax is:
\begin{verbatim}
replicate(n,expr)
\end{verbatim}
\subsection{Sampling from normal dsitribution}
If $x_i\sim N(0,1)$,  for $i=1\cdots n,$then $\bar{x}\sim N(0,\frac{1}{n}$. Note that $n$ is not an argument of \texttt{replicate()}. We can express this fact using the function \texttt{replicate()} for $n=10$, in the following \textbf{R} command. This command will simulate $n=10$ random observations from $N(0,1)$ and then computes its mean. This process is repeated $1000$ times, resulting $1000$ sample means from $N(0,1)$. We can verify the generated sample numerically as well as graphically.

<<replicate_norm01, message=FALSE, warning=FALSE>>=
xbar<-replicate(1000,mean(rnorm(10)))
mean(xbar) # zero
sd(xbar) #1/sqrt(10)
@
\subsubsection{Graphical illustration of sampling from $N(0,1)$}
<<graphic_samplingN01,message=FALSE,warning=FALSE,fig.cap="Sampling distribution from N(0,1) for a sample of size 10.">>=
xbar<-replicate(1000,mean(rnorm(10)))
hist(xbar,prob=TRUE)
curve(dnorm(x,mean = 0,sd=1/sqrt(10)),add=TRUE,lwd=1)
@
\section{Central limit theorem (CLT)}
If $x_i\sim (\mu,\sigma^2)$ for $i=1,\cdots,n$ $\Rightarrow  \bar{x}\stackrel{L}\rightarrow N(\mu,\frac{\sigma^2}{n})$. This is known as \emph{central limit theorem}. Note that in \textbf{CLT} even if parent population is not known the sampling distribution of mean can be approximated by a normal distribution $N(\mu,\sigma^2/n)$. 

\subsection{CLT when parent is \emph{standard exponential}}
Suppose that parent population is standard exponential, and a sample of size $10$ is taken to see the sampling behavior of the sample mean.This means that:
\begin{equation}
f(x)=\lambda e^{-\lambda x},\;
E(x)=\frac{1}{\lambda},\;
Var(x)=\frac{1}{\lambda^2},\;
SD(x)=\frac{1}{\lambda}
\end{equation}
Thus for standard exponential 
\begin{equation}
f(x)=e^{-x},\; mean=1,\;sd=1,\;
E(\bar{x})=n,\; SD(\bar{x})=\frac{1}{\sqrt{n}}
\end{equation}
Using CLT one can approximate the sampling distribution of sample mean by a normal distribution $N(mean=1,sd=1/\sqrt{10})$.  Note that exact distribution of sample mean is gamma distribution with $shape=10$ and $rate=10$. Keeping in view these facts we sue the following set of \textbf(R) commands:
<<cltexp10,message=FALSE,warning=FALSE,fig.cap="Comparison of exact and approximate sampling distributions of sampled mean (n=10) from standard exponential.">>=
#simulate 1000 sample means of size 10 from dexp(x,rate=1)
xbare10<-replicate(1000,expr=mean(rexp(10)))
mean(xbare10) #mean=1
sd(xbare10) #1/sqrt(10)
hist(xbare10,prob=TRUE,ylim=c(0,1.4))
curve(dnorm(x,1,1/sqrt(10)),add=TRUE)
curve(dgamma(x,shape=10,rate=10),add=TRUE,lty=2)
legend("topright",legend = c("Normal approximation","Exact"),lty=c(1,2))
@
Note that approximation is not excellent as $n=10$ only. See what happens when $n=100$. We expect that it will improve the approximation. Whole story can bee seen in the following set of commands:
<<cltexp100,message=FALSE,warning=FALSE,fig.cap="Comparison of exact and approximate sampling distributions of sampled mean (n=100) from standard exponential.">>=
#simulate 1000 sample means of size 100 from dexp(x,rate=1)
xbare100<-replicate(1000,expr=mean(rexp(100)))
mean(xbare100) #mean=1
sd(xbare100) #1/sqrt(100)
hist(xbare100,prob=TRUE,ylim=c(0,4))
curve(dnorm(x,1,1/sqrt(100)),add=TRUE)
curve(dgamma(x,shape=100,rate=100),add=TRUE,lty=2)
legend("topright",legend = c("Normal approximation","Exact"),lty=c(1,2))
@

\section{Student's t distribution with $5$ dgrees of freedom}
If $$t=\frac{z}{\sqrt{x/k}}\sim t_k$$
then it is termed as Students't distribution with $k$ degrees of freedom. It may be noted that $z\sim N(0,1)$ and $x\sim \chi_k^2$, that is $\chi^2$ with $k$  degrees of freedom. It must be noted that both $z$ and $x$ are independent random variables. We implement this sampling distribution with $k=5$ degrees of freedom as:
<<t5,message=FALSE,warning=FALSE,fig.cap="Students,t distribution with 5 degrees of freedom">>=
t5<-replicate(n=1000,expr=rnorm(1)/sqrt(rchisq(1,df=5)/5))
hist(t5,prob=TRUE,ylim=c(0,0.4),xlim=c(-5,5))
curve(dt(x,df=5),add=TRUE)
@
Note that histogram is constructed on simulated values using \texttt{replicate()} whereas density curve is exact $t$ with $5$ degrees of freedom. However, both are matching, which is an indication of the fact that both follow $t$ distribution with $5$ degrees of freedom.
\subsection{$\chi_k^{2}$ distribution with $k$ degrees of freedom}
It is a well known fact that if $z\sim N(0,1)$ then $z^2\sim \chi_1^{2}$, that is $\chi^2$ distribution with single degree of freedom. Similarly, $\chi_5^{2}$ distribution with $5$ degrees of freedom is defined as if $$z_i\sim N(0,1),\;i=1,\cdots,5$$ then
$$\Sigma_{i=1}^{i=5}z_i^2\sim \chi_5^{2}$$
This fact can be implemented using \textbf{R} commands as:
<<chisquare5,message=FALSE,warning=FALSE,fig.cap="Chi-square  sampling distribution with 5 degrees of freedom.">>=
chisq5<-replicate(1000,expr=sum(rnorm(5)^2))
hist(chisq5,prob=TRUE)
curve(dchisq(x,df=5),add=TRUE)
@
It is evident from above figure that simulated and exact curve are very close, depicting the fact that both represent the same distribution.
\section{Exercise on $F(2,12)$}
Define $F$ distribution with $2$ and $12$ degrees of freedoms. Implement that using the \texttt{replicate()} function, and support your findings graphically.
\section{Integration using simulation for \emph{beta} distribution}
Compute $E(X)$ where $x\sim beta(shape1=3,shape2=4)$ using simulation and verify that by using \texttt{integrate()} function of \texttt{R}.
<<rbeta34,message=FALSE,warning=FALSE>>=
x<-rbeta(10000,shape1=3,shape2=4)
mean(x) #(3/(3+4))
#Verification using integrate
# define a function for integrand of E(x)
f1<-function(x,shape1,shape2) x*dbeta(x,shape1,shape2)
integrate(f1,shape1=3,shape2=4,lower=0,upper=1)
@
Note that there is excellent agreement between the two results obtained from simulation as well as integration.
\section{Fitting of linear, quadratic, exponential, log-log models}
The \texttt{ironslag} data available with the \textbf{DAAG} package is a data frame which has $53$ rows and $2$ columns on measurements of iron contents obtained from iron slags, by using two methods, \emph{chemical} and \emph{magnetic} . A scatter plot of data suggests that chemical and magnetic variables are positively correlated, but the relation may not be \emph{linear}. From the plot, it appears that a quadratic polynomial, or possibly an exponential or logarithmic model might fit the data better than a line. Thus, proposed models for $y$ (magnetic measurement) vs x (chemical measurement) are:
\begin{equation}
M_1\;Linear:\quad y=\beta_0+\beta_1x+e
\end{equation}
\begin{equation}
M_2\;Quadratic:\quad y=\beta_0+\beta_1x+\beta_2x^2+e
\end{equation}
\begin{equation}
M_3\;Exponential:\quad log(y)=log(\beta_0)+\beta_1x+e\Rightarrow y=\beta_0e^{\beta_1x}e^{error}
\end{equation}
\begin{equation}
M_4\;Log-Log:\quad log(y)=\beta_0+\beta_1log(x)+e\Rightarrow y=e^{\beta_0}x^{\beta_1}e^{error}
\end{equation}
\subsection{The \texttt{ironslag} data with \textbf{DAAG}}
The package \textbf{DAAG} contains the \texttt{ironslag} data frame which can obtained from the package as:
<<slag1,message=FALSE,warning=FALSE>>=
library(DAAG)
data(ironslag)
require(knitr)
kable(head(ironslag),caption="Iron slag data on two measurements")
@
\subsection{Fitting of four models}
<<fourmodels4,message=FALSE,warning=FALSE,fig.cap="Fitting of four different models for iron slag data">>=
opar<-par(mfrow=c(2,2))
x<-seq(10,40,.1) #sequence in the range of chemical
#Fit linear model M1 
M1<-lm(magnetic~chemical,data=ironslag)
plot(magnetic~chemical,data=ironslag,main="Linear",pch=16)
yhat1<-M1$coef[1]+M1$coef[2]*x
lines(x,yhat1,lwd=2)
# Fit quadratic model M2
M2<-lm(magnetic~chemical+I(chemical^2),data=ironslag)
plot(magnetic~chemical,data=ironslag,main="Quadratic",pch=16)
yhat2<-M2$coef[1]+M2$coef[2]*x+M2$coef[3]*x^2
lines(x,yhat2,lwd=2)
# Fit Model M3
M3<-lm(log(magnetic)~chemical,data=ironslag)
plot(magnetic~chemical,main="Eponential",pch=16,data=ironslag)
logyhat3<-M3$coef[1]+M3$coef[2]*x
yhat3<-exp(logyhat3)
lines(x,yhat3,lwd=2)
# fit Model M4
M4<-lm(log(magnetic)~log(chemical),data=ironslag)
plot(log(magnetic)~log(chemical),main="Log-Log",data=ironslag,pch=16)
logyhat4<-M4$coef[1]+M4$coef[2]*log(x)
lines(log(x),logyhat4,lwd=2)
par(opar) #go to default settings
@
It is a different issue that which model fits best. Form graphics it seems that \emph{Quadratic} model fits best. However, this is totally subjective on the basis of graphics. There are standard objective methods to compare different models. These are the issues which will be covered at higher level courses. 

\section{Fitting of distributions using ML methods}
Fitting of distributions is a very simple task in \textbf{R}. We shall use the function \texttt{optim()} for maximum likelihood estimation. This function returns negative of the \emph{Hessian} matrix provided negative of log-likelihood is the objective function to be minimized. It is a well known fact that if $l_i(\theta)$ is the loglikelihood for $i^{th}$ observation and $l(\theta)$ is the loglikelihood for $n$ observations, then
\begin{equation}
l(\theta)=\Sigma_{i=1}^{n}l_i(\theta)\; for\; i=1,\cdots,n
\end{equation}
Thus maximum likelihood estimate is obtained by 
\begin{equation}
min(-l(\theta))
\end{equation}
and \emph{Hessian} matrix is $-l''(\theta)|_{\theta=\hat{\theta}}=I(\hat{\theta})$, which is termed as observed Fisher's information matrix. It may be noted that that asymptotic variance of $\hat{\theta}$ is $I^{-1}(\hat{(\theta)})$. This implies that standard error of $\hat{\theta}$ is 
$$\sqrt(diag(I^{-1}(\hat{\theta})))$$. Thus, $100(1-\alpha)\%$ confidence interval for $\theta$ is 
$$\hat{\theta}\pm qnorm(\alpha/2,lower.tail=FALSE)*se(\hat{\theta})$$
where $$se(\hat{\theta})=\sqrt(diag(solve(I(\hat{\theta}))))$$.
\subsection{Introduction to \texttt{optim()} of R}
The function \texttt{optim(par,fn,hessian=TRUE,...)} is meant for nonlinear unconstrained optimization (preferarably minimization). It requires guess value of the parameter (argument \texttt{par}) and objective function $-l(\theta)$. The argument \texttt{hessian=TRUE} is required to return \emph{Hessian} matrix at the optimum point. Three dots ($\cdots$) at the end in arguments stand for arguments which are not specified.
\subsection{Fitting of exponential distribution}
Note that $$f(y|\lambda)=\lambda e^{-\lambda y}=dexp(y,rate=\lambda)$$
This implies that loglikelihood for exponential with $rate=\lambda$ is
$$l_i(\lambda)=log(f(y|\lambda))=log(\lambda)-\lambda y_i\quad for\;i=1,\cdots,n,$$
This implies that $$l(\lambda)=\Sigma l_i(\lambda)=\Sigma_{i=1}^{i=n}(log(\lambda)-\lambda y_i)=sum(dexp(y,\lambda,log=TRUE))$$

<<>>=
# fit exponential distribution
set.seed(1)
y<-rexp(20,rate=0.03) # simulate 20 random observations from exponential distribution with rate 0.03.
y
# define negative of loglikelihood of exponential distribution
nlle<-function(theta,y){
  ll<-dexp(y,rate=theta,log=TRUE)
  ll<-sum(ll)
  return(-ll)
}
dump("nlle",file="nlle.txt") #save it
# Fitting with optim
M1<-optim(par=0.01,fn=nlle,hessian=TRUE,method="L-BFGS-B",lower=0.001,y=y)
M1
MLE<-M1$par #ML estimator
se<-sqrt(diag(solve(M1$hessian))) # SE of thetahat
MLE
se
out<-rbind(MLE,se)
colnames(out)<-c("rate")
out
@
\subsection{Fitting of Weibull model}
Weibull distribution is a very popular distribution for fitting of reliability and survival data. It has a \emph{shape} and a \emph{scale} parameter. It is fitted as:
<<weib1,message=FALSE,warning=FALSE>>=
# define a function which returns negative of lolglikelihood of weibull.
set.seed(1)
y<-rweibull(30,shape=1.5,scale=50) # simulate 30 observations 
##define -logliklihood of Weibull,-l(theta), theta=c(shape,scale)

nllw<-function(theta,y){
  ll<-dweibull(y,shape=theta[1],scale=theta[2],log=TRUE)
  ll<-sum(ll)
  return(-ll)
}
dump("nllw",file="nllw.txt") #save it
##Fit it
M2<-optim(par=c(1,30),fn=nllw,hessian=TRUE,method="L-BFGS-B",
          lower=c(0.1,2),y=y)
MLE=M2$par
se=sqrt(diag(solve(M2$hessian)))
out<-rbind(MLE,se)
colnames(out)<-c("shape","scale")
out
@
It may be noted that fitted values are in close agreement with the true values, which shows the strength of fitting procedure.
\subsection{Assignment}
Describe this fitting of Weibull, graphically.

\subsection{Fitting with \texttt{fitdistr()} of \textbf{MASS}}
A very powerful function \texttt{fitdistr()} is available with the \textbf{MASS} package. A large number of distributions can be fitted using this function. This includes, normal, Poisson, t, gamma etc. It can be reported as \texttt{fitdistr(x,densfun,start,...)}, where $x$ stands for the data vector to be fitted, $densfun$ stands for the name of the density to be fitted, $start$ stands for guess vales to start the iterations. Let us begin with $gamma$ distribution:
<<gamm1,message=FALSE,warning=FALSE>>=
library(MASS)
set.seed(123)
x <- rgamma(100, shape = 5, rate = 0.1)
fitdistr(x, "gamma")
## now do this directly with more control.
fitdistr(x, dgamma, list(shape = 1, rate = 0.1), lower = 0.001)
@
\subsection{Assignments}
\begin{enumerate}
\item Fit gamma distribution using \texttt{optim()} as discussed for Weibull and compare your results with the results obtained using \texttt{fitdistr()}.
\item Simulate 30 observations from a Poisson distribution with mean 10 and fit that using \texttt{fitdistr()}.
\end{enumerate}
\section{Quantile Quantile plots}
Quantile-quantile plots (known as QQ plots) are a type of scatter plot used to compare the distributions of two groups or to compare a sample with a reference distribution. The line $y=x$ implies both are same.

\subsection{The function \texttt{qqplot()}}
The \textbf{R} function \texttt{qqplot()} is meant for graphic comparisons of the two data sets or one data set with a reference distribution. 
<<qq1,message=FALSE,warning=FALSE>>=
# compare two standard normal distributions
x<-rnorm(1000) #x~N(0,1)
a<-rnorm(1000)
opar<-par(mfrow=c(2,2))
qqplot(x,a,main=" a and x are same")
#comapre N(0,1) with N(3,2)
b<-rnorm(1000,mean=3,sd=2) #b~N(mean=3,sd=2)
qqplot(x,b,main="b is rescaled x")
#Compare Student's t with df=2 with N(0,1)
c1<-rt(1000,df=2) 
qqplot(x,c1,main="c1 has heavier tails")
#Compare lognormal with normal
d<-exp(rnorm(1000)) #simulation from lognormal
qqplot(x,d,main="d is skewed to right")
par(opar) #return to original settings
@
Note that in the last two figures, distributions are not matching and line $y=x$ is not satisfied. Moreover, if you want to compare two real data vectors $x$ and $y$, they can be compared in the same manner.
\subsection{Exercise: qqplot}
Compare the distribution height and weight of students of B Sc Final, using \texttt{qqplot()}.
\subsection{Normal probability plot---\texttt{qqnorm()}}
The function \texttt{qqnorm} compares Normal distribution with a data set. A reference line can also be added by \texttt{qqline()}. 
<<qqn2,message=FALSE,warning=FALSE,fig.cap=c("Normal probability plot.")>>=
opar<-par(mfrow=c(1,2))
x<-rnorm(1000)
qqnorm(x)
qqline(x)
xe<-rexp(1000,rate=800)
qqnorm(xe,main="Exponential with rate=800")
qqline(xe)
par(opar)
@
Note that data $xe$ which follows exponential distribution is not matching with the normal distribution. This is evidenced from the fact that in case of exponential, reference line is not matching with plot.
\section{Import Export}
\textbf{R} has very powerful interface with other systems. One can import data from other packages by using the package \textbf{foreign}. Similarly, one can save output to the systems too by using the function \texttt{write.foreign()}. A set of commands follow:
<<foreign1,message=FALSE,warning=FALSE>>=
library(foreign)
datafile<-tempfile()
codefile<-tempfile()
write.foreign(trees,datafile,codefile,package="SPSS")
file.show(datafile)
file.show(codefile)
@
\subsection{Reading data from SPSS}
We can read a data from \textbf{SPSS} into \textbf{R} very easily as:
<<readSPSS1,warning=FALSE,message=FALSE>>=
sav<-system.file("files","electric.sav",package="foreign")
dat<-read.spss(file=sav)
str(dat) #it is a listed structure
datF<-read.spss(file=sav,to.data.frame = TRUE)
str(datF) # now it is a data frame
head(datF)
tail(datF)
@
\subsection{Reading from stata}
One can read and a \texttt{stata} file into \textbf{R}. An illustration is made with `swiss` data available with the package \textbf{datasets}. The functions `read.dta()` and `write.dta()`. 
<<swiss1,message=FALSE,warning=FALSE>>=
head(swiss) #see the header part
tail(swiss) #see the tail part
str(swiss) #see the structure of the data
#write the data frame `swiss` to a stata file `swiss2.dta`
require(foreign)
write.dta(swiss,file="swiss2.dta")
# Read this data into R as data frame
swiss2<-read.dta("swiss2.dta")
head(swiss2)
@
The best strategy is that you download the `.dta` file into the working folder and then use the function `read.dta()` as we did above. 
\subsection{Reading data from Excel}
\textbf{MSExcel} is a very popular spread sheet software for entering data. Most of the users come with the data entered into \textbf{Excel} and want data analysis done in \textbf{R}. Following steps are required:
\begin{verbatim}
1. Enter your data into Excel's spread sheet.

2. Save it as comma delimited values, say "D1.csv".

3. Read it into R as:  D1<-read.csv("D1.csv")

4. Save it into R as: write.csv(D1,file="D2.csv",row.names=F)
\end{verbatim}
Note that when you enter data into Excel's spread sheet, the first row should be the variable names. These names should not be fractured names. For example \texttt{Plant Height} is a fractured nams, it should be \texttt{PlantHeight}. As an example, consider the following data need be entered:
\begin{verbatim}
Height Weight Gender
165     55    Male
160     52    Female
178     70    Male
166     65    Male
169     60    Male
163     58    Female  
\end{verbatim}
We will enter into Excel and save it as \texttt{D1.csv} file, and then read it by using data frame object \texttt{D1}.
<<ExcelD1,message=FALSE,warning=FALSE>>=
D1<-read.csv("D1.csv")
D1
write.csv(D1,file="D2.csv",row.names=FALSE)
D2<-read.csv("D2.csv")
D2
# get summary of data
summary(D2)
@
\subsection{Reading a data from internet}
One can read a data which is available on internet and its path is \texttt{"https://goo.gl/UDv12g"}. One can read this data as:
<<df1>>=
DF1<-read.csv("https://goo.gl/UDv12g")
head(DF1)
@
This a sales data. Similarly, a data available with\\ \texttt{"http://www.jaredlander.com/data/Tomato\%20First.csv"}\\  can be read into \textbf{R} as:
<<tomatoFirst1>>=
theUrl<-"http://www.jaredlander.com/data/Tomato%20First.csv"
tomato<-read.csv(file=theUrl)
head(tomato)
#If we want to use read.table(), then
tomato1<-read.table(file=theUrl,header = TRUE,sep=",")
head(tomato1,n=3)
@
Note that when you run these commands, your system must be connected with internet. This is the requirement for generating the documents also. Alternatively, you can save these data set into your working directory and use \texttt{read.csv()} to read them.
\chapter{Hypothesis testing using R}
\begin{verbatim}
z-test
y~N(mu,sigma^2)
H0: mu=mu0=140
H1: mu not eqaul to 140.
 and sigma^2 is known, the test criteria is z-test. Hwoever, if sigma^2 is not known the, test criteria is t-test. 
z=(ybar-mu0)/(sigma/sqrt(n))~N(0,1)
y<-c(168,170,185,172,169,158,170,175)
sigma=5
Since sigma is known, we have to use z-test.
ztest=(mean(y)-140)/5/sqrt(length(y))
qnorm(.05/2,lower.tail=FALSE) # tabulated value
1.If calculated value is greater than tabulated, we reject H0.
2. If p-value is less than 0.05, we reject H0.
pvalue<-2*(pnorm(abs(z),lower.tail=FALSE))
pvalue=2.62222e-68
which is less than 0.05, hence we reject H0.

3. If sigma is unknown, then we use t-test.

t=(mean(y)-140)/(sd(y)/sqrt(length(y)))
It calculated t is greater thn tabulated t, we reject H0
qt(0.05/2,df=length(y)-1,lower.tail=FALSE) # tabulated value

However, if pvalue is less than 0.05, we reject H0.
pvalue<-2*(pt(abs(t),df=length(y)-1,lower.tail=FALSE))

## ttest for two samples
## t-test for comparing means of two normal populations
## If y1~N(mu1,sigma1^2)
and y2~N(mu2,sigma2^2)
then to test H0: mu1=mu2
vs
             H1:mu1!=mu2

provided sigma1^2=sigma2^2
then test criteria is t-test

t=(mean(y1)-mean(y2))/(sp*sqrt(1/n1+1/n2)~Student_t(n1-1+n2-1)
where 
sp^2=((n1-1)s1^2+(n2-1)s2^2)/((n1-1)+(n2-1))
sp=sqrt(sp^2)
We reject H0 if |t|>qt(alpha/2,df=(n1-1+n2-1),lower.tail=FALSE)
or we reject H0 if 
pvalue=2*(pt(abs(t),df=n1-1+n2-1,lower.tail=FALSE))
is less than 0.05.

## write a function to implement t-test for two samples
twot<-function(y1,y2){
#ttest  for two samples
#assume sigma1^2=sigma2^2
# y1 and y2 are two data vectors
n1<-length(y1)
n2<-length(y2)
ybar1=mean(y1)
ybar2=mean(y2)
var1=var(y1)
var2=var(y2)
dferror=n1+n2-2
sp2=((n1-1)*var1+(n2-1)*var2)/dferror
t=(ybar1-ybar2)/sqrt(sp2*(1/n1+1/n2))
pvalue=2*(pt(abs(t),dferror,lower.tail=FALSE))
out<-list(mean1=ybar1,mean2=ybar2,var1=var1,var2=var2,t=t,pvalue=pvalue)
return(out)
}
dump("twot",file="twot.txt")

## Take a data 
set.seed(1)
y1<-rnorm(10,8,2) # 10 observations from N(8,sd=2)
set.seed(1)
y2<-rnorm(12,20,3)# 12 observations from N(20,sd=3)
#Use twot
twot(y1=y1,y2=y2)
## We reject H0 as pvalue is less than 0.05.
## This means on average menas of two populations differ significantly.

## Exercise: 
1. Write a function for one sample t-test like above. Use that to test a data discussed in a standard book like Hogg and Tanis (2009). 

## Moore D F, Mc Cabe, and Craig(2008): Introduction to the Practice of Statistics. Sixth edition
\end{verbatim}

\end{document}
